---
title: 大数据量处理
tags: design
---

## 怎么做大批次数据的处理？

### 1. 两个超大文件核对
假设现在生产上有两个文件，分别都是20GB的大小，但是A文件是完整的，B文件是缺失的，文件内容都是有序增长的ID。现在我们怎么设计程序找出B文件中缺少的ID？
让我详细解释如何处理这个大文件比对的问题：

### 1. 分片读取方案
```java
public class FileDiffProcessor {
    private static final int BUFFER_SIZE = 10 * 1024 * 1024; // 10MB缓冲区
    
    public void findMissingIds(String fileA, String fileB) {
        try (BufferedReader readerA = new BufferedReader(new FileReader(fileA), BUFFER_SIZE);
             BufferedReader readerB = new BufferedReader(new FileReader(fileB), BUFFER_SIZE)) {
            
            String lineA = readerA.readLine();
            String lineB = readerB.readLine();
            
            while (lineA != null && lineB != null) {
                long idA = Long.parseLong(lineA);
                long idB = Long.parseLong(lineB);
                
                if (idA < idB) {
                    // 发现缺失ID
                    System.out.println("Missing ID: " + idA);
                    lineA = readerA.readLine();
                } else if (idA > idB) {
                    lineB = readerB.readLine();
                } else {
                    lineA = readerA.readLine();
                    lineB = readerB.readLine();
                }
            }
            
            // 处理A文件剩余的ID
            while (lineA != null) {
                System.out.println("Missing ID: " + lineA);
                lineA = readerA.readLine();
            }
        }
    }
}
```

### 2. 多线程并行处理
```java
public class ParallelFileDiffProcessor {
    private static final long CHUNK_SIZE = 1024 * 1024 * 1024L; // 1GB分片
    
    public void processInParallel(String fileA, String fileB) {
        // 计算文件分片
        long totalSize = new File(fileA).length();
        int chunks = (int) (totalSize / CHUNK_SIZE) + 1;
        
        ExecutorService executor = Executors.newFixedThreadPool(
            Math.min(chunks, Runtime.getRuntime().availableProcessors())
        );
        
        // 提交分片任务
        for (int i = 0; i < chunks; i++) {
            final long start = i * CHUNK_SIZE;
            final long end = Math.min((i + 1) * CHUNK_SIZE, totalSize);
            
            executor.submit(() -> processChunk(fileA, fileB, start, end));
        }
        
        executor.shutdown();
    }
    
    private void processChunk(String fileA, String fileB, long start, long end) {
        // 处理指定范围的文件内容
        try (RandomAccessFile rafA = new RandomAccessFile(fileA, "r");
             RandomAccessFile rafB = new RandomAccessFile(fileB, "r")) {
            
            rafA.seek(start);
            // 找到对应范围的起始ID并处理
            // ... 处理逻辑
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```

### 3. 内存映射方案
```java
public class MappedFileDiffProcessor {
    private static final int MAPPING_SIZE = 1024 * 1024 * 1024; // 1GB映射
    
    public void processByMapping(String fileA, String fileB) throws IOException {
        try (FileChannel channelA = FileChannel.open(Paths.get(fileA), StandardOpenOption.READ);
             FileChannel channelB = FileChannel.open(Paths.get(fileB), StandardOpenOption.READ)) {
            
            long sizeA = channelA.size();
            long position = 0;
            
            while (position < sizeA) {
                long remainingSize = sizeA - position;
                int mappingSize = (int) Math.min(MAPPING_SIZE, remainingSize);
                
                MappedByteBuffer bufferA = channelA.map(
                    FileChannel.MapMode.READ_ONLY, position, mappingSize);
                MappedByteBuffer bufferB = channelB.map(
                    FileChannel.MapMode.READ_ONLY, position, mappingSize);
                
                compareBuffers(bufferA, bufferB);
                position += mappingSize;
            }
        }
    }
}
```

### 4. 实现建议

1. **选择合适的方案**
   - 内存受限：使用分片读取
   - 多核CPU：使用并行处理
   - 大内存服务器：使用内存映射

2. **优化建议**
```java
public class OptimizedProcessor {
    // 使用批量写入结果
    private static final int BATCH_SIZE = 1000;
    private List<Long> missingIds = new ArrayList<>(BATCH_SIZE);
    
    private void writeMissingIds() {
        if (missingIds.size() >= BATCH_SIZE) {
            // 批量写入文件
            writeToFile(missingIds);
            missingIds.clear();
        }
    }
    
    // 使用压缩存储结果
    private void writeCompressed(List<Long> ids) {
        try (GZIPOutputStream gzipOut = new GZIPOutputStream(
                new FileOutputStream("missing_ids.gz"))) {
            // 压缩写入
        }
    }
}
```

3. **异常处理**
```java
public class ErrorHandler {
    private void handleProcessingError(long position, Exception e) {
        // 记录错误位置
        log.error("Processing error at position: " + position, e);
        // 保存处理进度
        saveProgress(position);
        // 支持断点续处理
        resumeFromPosition(position);
    }
}
```

### 5. 性能考虑

1. **内存使用**
   - 控制缓冲区大小
   - 及时释放资源
   - 使用NIO减少复制

2. **IO优化**
   - 使用缓冲读取
   - 批量写入结果
   - 考虑使用SSD

3. **并行处理**
   - 根据CPU核心数设置线程
   - 合理分片避免过多开销
   - 注意结果合并开销

这个方案可以高效处理大文件比对，同时保持较低的内存占用。根据实际环境选择合适的实现方式，可以达到最佳的处理效果。

## 怎么从一个超大文件中找出重复最多的十个ID？

你的问题很有挑战性！让我用通俗的语言，像讲故事一样，逐步分析如何在一台只有 **2GB 内存** 的电脑上，从几亿条搜索日志中选出 **搜索热度最高的十个关键词**。假设日志存储在多台机器上（分布式存储），我们得把数据拉过来处理，但内存有限，所以得聪明点。以下是思路和方案：

---

### 1. 问题背景
- **数据规模**：
  - 几亿条搜索日志，每条可能包含时间戳、用户 ID、搜索关键词等，假设每条日志几十字节，总大小可能是几十 GB 到几百 GB。
- **资源限制**：
  - 2GB 内存（实际可用可能不到 2GB，因为系统占一部分）。
  - 单机处理，没法直接把所有数据塞进内存。
- **目标**：
  - 统计每个关键词出现的次数，找出 Top 10。
- **比喻**：
  - 像在小本子上记账，但账单有几亿页，桌子（内存）只能摊开几页，得想办法分批记。

---

### 2. 面对的挑战
- **内存不够**：
  - 几亿条日志没法一次性加载到 2GB 内存里。
  - 用 HashMap 存关键词和计数，假设 1 亿个唯一关键词，每个关键词平均 10 字节，计数用 4 字节，内存就要 1.4GB，还不算 HashMap 开销，撑不住。
- **分布式数据**：
  - 日志分散在多台机器，得取回来处理。
- **单机瓶颈**：
  - CPU 和磁盘 IO 也有限，不能太慢。

---

### 3. 解决思路：分而治之 + 外部排序
既然内存不够，我们得把大问题拆成小块，分批处理，最后合并结果。核心是 **分片统计 + Top K 选择**，用以下步骤：

#### (1) 分片读取数据
- **怎么做**：
  - 从多台机器上分批拉日志，每次拉一小部分（比如 1GB），能装进内存处理。
  - 用流式读取（比如 Java 的 BufferedReader），边读边处理，不全存内存。
- **比喻**：
  - 像从仓库一车车拉账单，每次只拉一小堆，记完扔掉，再拉下一堆。

#### (2) 分片统计关键词
- **怎么做**：
  - 对每批数据，统计关键词出现次数，用 HashMap 存（内存够用，因为每批小）。
  - 每批处理完，把结果写到磁盘文件（比如 "shard1.txt"），格式是 "关键词:次数"。
- **细节**：
  - 假设每批 1000 万条日志，唯一关键词可能几十万，HashMap 占用几百 MB，2GB 内存撑得住。
- **比喻**：
  - 每车账单记完，抄到一个小账本（磁盘文件），然后清桌子，记下一车。

#### (3) 合并分片结果
- **问题**：
  - 磁盘上有了几十个分片文件（比如 50 个 shardX.txt），每个文件有关键词和次数，但内存装不下所有文件。
- **怎么做**：
  - 用 **外部归并排序** 或 **最小堆** 合并：
    1. 每个分片文件开一个文件指针，读一行（关键词:次数）。
    2. 用一个小顶堆（大小 10），存当前 Top 10 候选。
    3. 遍历所有分片，每次从某个文件读下一行，更新堆。
- **内存使用**：
  - 堆里 10 个元素，每个元素关键词 + 次数几十字节，总共几百字节，内存轻松hold住。
- **比喻**：
  - 像从一堆小账本里挑最大的 10 个数字，每次只看一本的一行，记个小排行榜（堆），不断更新。

#### (4) 输出 Top 10
- **怎么做**：
  - 遍历完所有分片，堆里剩下的就是全局 Top 10。
- **比喻**：
  - 最后排行榜定下来，抄到大纸上（结果）。

---

### 4. 具体实现步骤
#### 假设条件
- 日志格式：每行一个关键词（简化问题，实际可能要解析）。
- 总日志 5 亿条，每条 50 字节，总大小约 25GB。
- 分散在 10 台机器，每台 2.5GB 数据。

#### 步骤
1. **分片拉取**：
   - 每次从一台机器拉 500MB 数据（约 1000 万条），分 5 次拉完一台，总共 50 个分片。
   - 用 HTTP 或 SSH 拉数据，存到本地磁盘。
2. **分片统计**：
   - 对每个 500MB 文件：
     - 用 HashMap 统计关键词次数。
     - 写到磁盘（比如 "shard1.txt"），格式 "keyword1:100"。
   - 内存：500MB 数据 + HashMap（假设 50 万关键词，约 700MB），共 1.2GB，可行。
3. **合并 Top 10**：
   - 开 50 个文件指针（占用内存几十 KB）。
   - 用小顶堆（10 个元素，几十字节）。
   - 遍历所有文件行，堆动态更新。
4. **结果**：
   - 堆里 10 个关键词 + 次数就是答案。

#### 时间复杂度
- 分片统计：O(N)，N 是日志总数。
- 合并：O(M * log K)，M 是总关键词数（假设 1 亿），K 是 10。
- 总时间：线性为主，够快。

#### 空间复杂度
- 内存：每批 1.2GB + 堆几十字节。
- 磁盘：25GB（原始数据）+ 几 GB（分片文件），假设有足够磁盘。

---

### 5. 优化：哈希分片
如果关键词种类太多（比如 1 亿个），分片文件还是太大，可以用哈希分片：
- **怎么做**：
  - 对关键词哈希（比如 hash(keyword) % 100），分成 100 个桶。
  - 每个桶单独统计和合并，最后再从 100 个桶的 Top 10 里挑全局 Top 10。
- **好处**：
  - 每个桶数据量小，处理更快。
- **比喻**：
  - 把账单按首字母分成 100 堆，每堆挑 Top 10，再从 1000 个候选里挑最终 Top 10。

---

### 6. 通俗总结
- **方法**：
  - 分批拉数据 -> 每批记账 -> 磁盘存小账本 -> 用排行榜挑 Top 10。
- **为啥行**：
  - 内存小，分片干；堆小，Top 10 随便挑。
- **比喻**：
  - 像在小桌子上数几亿张选票，每次数一小摞，最后从每摞的前几名里挑总冠军。