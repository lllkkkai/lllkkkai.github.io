---
layout: post
category: Java
---

## 数据结构
HashMap由数组+链表实现对数据的存储，JDK1.8中链表长度大于8转换为红黑树.

- 为什么使用红黑树替换链表？
> 随着存储数据的增加，链表的长度会持续增长，查询效率会越来越低，通过转变成红黑树可以提升查询的效率
> 
> 红黑树增删改查**时间复杂度都是O(logn)** 比链表好

HashMap中的<key, value>元素被封装成Node对象存储
```Java
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;

    Node(int hash, K key, V value, Node<K,V> next) {
        this.hash = hash;
        this.key = key;
        this.value = value;
        this.next = next;
    }

    public final K getKey()        { return key; }
    public final V getValue()      { return value; }
    public final String toString() { return key + "=" + value; }

    public final int hashCode() {
        return Objects.hashCode(key) ^ Objects.hashCode(value);
    }

    public final V setValue(V newValue) {
        V oldValue = value;
        value = newValue;
        return oldValue;
    }

    public final boolean equals(Object o) {
        if (o == this)
            return true;
        if (o instanceof Map.Entry) {
            Map.Entry<?,?> e = (Map.Entry<?,?>)o;
            if (Objects.equals(key, e.getKey()) &&
                Objects.equals(value, e.getValue()))
                return true;
        }
        return false;
    }
}
```

HashMap通过hash方法计算key的哈希码，然后通过(n-1)&hash公式（n为数组长度）得到key在数组中存放的下标。当两个key在数组中存放的下标一致时，数据将以链表的方式存储（哈希冲突，哈希碰撞）。我们知道，在链表中查找数据必须从第一个元素开始一层一层往下找，直到找到为止，时间复杂度为O(N)，所以当链表长度越来越长时，HashMap的效率越来越低。

为了解决这个问题，JDK1.8开始采用数组+链表+红黑树的结构来实现HashMap。当链表中的元素超过8个（TREEIFY_THRESHOLD）并且数组长度大于64（MIN_TREEIFY_CAPACITY）时，会将链表转换为红黑树，转换后数据查询时间复杂度为O(logN)。

红黑树的节点使用TreeNode表示：
```Java
static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
    TreeNode<K,V> parent;  // red-black tree links
    TreeNode<K,V> left;
    TreeNode<K,V> right;
    TreeNode<K,V> prev;    // needed to unlink next upon deletion
    boolean red;
    TreeNode(int hash, K key, V val, Node<K,V> next) {
        super(hash, key, val, next);
    }
    ...
}
```

几个重要变量
```Java
// 数组默认的初始化长度16
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;

// 数组最大容量，2的30次幂，即1073741824
static final int MAXIMUM_CAPACITY = 1 << 30;

// 默认加载因子值
static final float DEFAULT_LOAD_FACTOR = 0.75f;

// 链表转换为红黑树的长度阈值
static final int TREEIFY_THRESHOLD = 8;

// 红黑树转换为链表的长度阈值
static final int UNTREEIFY_THRESHOLD = 6;

// 链表转换为红黑树时，数组容量必须大于等于64
static final int MIN_TREEIFY_CAPACITY = 64;

// HashMap里键值对个数
transient int size;

// 扩容阈值，计算方法为 数组容量*加载因子
int threshold;

// HashMap使用数组存放数据，数组元素类型为Node<K,V>
transient Node<K,V>[] table;

// 加载因子
final float loadFactor;

// 用于快速失败，由于HashMap非线程安全，在对HashMap进行迭代时，如果期间其他线程的参与导致HashMap的结构发生变化了（比如put，remove等操作），直接抛出ConcurrentModificationException异常
transient int modCount;
```

加载因子也叫扩容因子，用于决定HashMap数组何时进行扩容。比如数组容量为16，加载因子为0.75，那么扩容阈值为16*0.75=12，即HashMap数据量大于等于12时，数组就会进行扩容。我们都知道，数组容量的大小在创建的时候就确定了，所谓的扩容指的是重新创建一个指定容量的数组，然后将旧值复制到新的数组里。扩容这个过程非常耗时，会影响程序性能。所以加载因子是基于容量和性能之间平衡的结果：

- 当加载因子过大时，扩容阈值也变大，也就是说扩容的门槛提高了，这样容量的占用就会降低。但这时哈希碰撞的几率就会增加，效率下降；
- 当加载因子过小时，扩容阈值变小，扩容门槛降低，容量占用变大。这时候哈希碰撞的几率下降，效率提高。
可以看到容量占用和性能是此消彼长的关系，它们的平衡点由加载因子决定，0.75是一个即兼顾容量又兼顾性能的经验值。

此外用于存储数据的table字段使用transient修饰，通过transient修饰的字段在序列化的时候将被排除在外，那么HashMap在序列化后进行反序列化时，是如何恢复数据的呢？HashMap通过自定义的readObject/writeObject方法自定义序列化和反序列化操作。这样做主要是出于以下两点考虑：

- table一般不会存满，即容量大于实际键值对个数，序列化table未使用的部分不仅浪费时间也浪费空间；
- key对应的类型如果没有重写hashCode方法，那么它将调用Object的hashCode方法，该方法为native方法，在不同JVM下实现可能不同；换句话说，同一个键值对在不同的JVM环境下，在table中存储的位置可能不同，那么在反序列化table操作时可能会出错。

所以在HashXXX类中（如HashTable，HashSet，LinkedHashMap等等），我们可以看到，这些类用于存储数据的字段都用transient修饰，并且都自定义了readObject/writeObject方法。
## put做了什么
```Java
// put方法源码如下：
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

// put方法通过hash函数计算key对应的哈希值，hash函数源码如下：
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

如果key为null，返回0，不为null，则通过(h = key.hashCode()) ^ (h >>> 16)公式计算得到哈希值。该公式通过hashCode的高16位异或低16位得到哈希值，主要从性能、哈希碰撞角度考虑，减少系统开销，不会造成因为高位没有参与下标计算从而引起的碰撞。

得到key对应的哈希值后，再调用putVal(hash(key), key, value, false, true)方法插入元素：

```Java
// 用final修饰限制任何子类重写
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    // 如果数组(哈希表)为null或者长度为0，则进行数组初始化操作，初始化也是调用resize方法实现的
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    // 根据key的哈希值计算出数据插入数组的下标位置，公式为(n-1)&hash
    if ((p = tab[i = (n - 1) & hash]) == null)
        // 如果该下标位置还没有元素，则直接创建Node对象，并插入
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        // 如果目标位置key已经存在，则直接覆盖
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        // 如果目标位置key不存在，并且节点为红黑树，则插入红黑树中
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            // 否则为链表结构，遍历链表，尾部插入
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    // 如果链表长度大于等于TREEIFY_THRESHOLD，则考虑转换为红黑树
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash); // 转换为红黑树操作，内部还会判断数组长度是否小于MIN_TREEIFY_CAPACITY，如果是的话不转换
                    break;
                }
                // 如果链表中已经存在该key的话，直接覆盖替换
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            // 返回被替换的值
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    // 模数递增
    ++modCount;
    // 当键值对个数大于等于扩容阈值的时候，进行扩容操作
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

1. 判断HashMap数组是否为空，是的话初始化数组（由此可见，在创建HashMap对象的时候并不会直接初始化数组）；
2. 通过(n-1) & hash计算key在数组中的存放索引；
3. 目标索引位置为空的话，直接创建Node存储；
4. 目标索引位置不为空的话，分下面三种情况：
    1. key相同，覆盖旧值；
    2. 该节点类型是红黑树的话，执行红黑树插入操作；
    3. 该节点类型是链表的话，遍历到最后一个元素尾插入，如果期间有遇到key相同的，则直接覆盖。如果链表长度大于等于TREEIFY_THRESHOLD，并且数组容量大于等于MIN_TREEIFY_CAPACITY，则将链表转换为红黑树结构；
5. 判断HashMap元素个数是否大于等于threshold，是的话，进行扩容(resize)操作。

```Java
// get方法较为简单，按熟悉检查数组、数组下标位置元素、顺着下一个节点继续检查（链表、红黑树）
public V get(Object key) {
    Node<K,V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}

final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    // 判断数组是否为空，数组长度是否大于0，目标索引位置下元素是否为空，是的话直接返回null
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & hash]) != null) {
        // 如果目标索引位置元素就是要找的元素，则直接返回
        if (first.hash == hash && // always check first node
            ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        // 如果目标索引位置元素的下一个节点不为空
        if ((e = first.next) != null) {
            // 如果类型是红黑树，则从红黑树中查找
            if (first instanceof TreeNode)
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            do {
            // 否则就是链表，遍历链表查找目标元素
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

## 1.8下的扩容

```Java
final Node<K,V>[] resize() {
    // 扩容前的数组
    Node<K,V>[] oldTab = table;
    // 扩容前的数组的大小和阈值
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    // 预定义新数组的大小和阈值
    int newCap, newThr = 0;
    if (oldCap > 0) {
        // 超过最大值就不再扩容了
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        // 扩大容量为当前容量的两倍，但不能超过 MAXIMUM_CAPACITY
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    // 当前数组没有数据，使用初始化的值
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    else {               // zero initial threshold signifies using defaults
        // 如果初始化的值为 0，则使用默认的初始化容量，默认值为16
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    // 如果新的容量等于 0
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr; 
    @SuppressWarnings({"rawtypes","unchecked"})
    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    // 开始扩容，将新的容量赋值给 table
    table = newTab;
    // 原数据不为空，将原数据复制到新 table 中
    if (oldTab != null) {
        // 根据容量循环数组，复制非空元素到新 table
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                // 如果链表只有一个，则进行直接赋值
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                    // 红黑树相关的操作
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else { // preserve order
                    // 链表复制，JDK 1.8 扩容优化部分
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        // 原索引
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        // 原索引 + oldCap
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    // 将原索引放到哈希桶中
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    // 将原索引 + oldCap 放到哈希桶中
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

hashmap的长度是2的n次幂，因为计算节点下标时，可以满足 hash%n == (n-1)&hash，利用位运算加快计算速度并且实现更少的hash碰撞

> 为什么扩容2倍？
>
> 哈希算法与索引计算:
> 
> HashMap 在存储键值对时，需要根据键的哈希值计算其在哈希表数组中的索引位置，通常使用的计算方式是 (n - 1) & hash，其中 n 是哈希表数组的长度，hash 是键的哈希值。这里使用按位与操作 & 替代取模运算 %，是因为按位与操作在性能上更高效。
当数组长度 n 是 2 的幂次方时，(n - 1) 的二进制表示会是一个全为 1 的数。例如，当 n = 8 时，n - 1 = 7，二进制表示为 0111。这样，(n - 1) & hash 的结果就等价于 hash % n，并且能均匀地分布键值对到数组的各个位置，减少哈希冲突的概率。
在扩容时将数组长度扩大为原来的两倍，新的长度依然是 2 的幂次方，这就保证了在扩容后仍然可以使用 (n - 1) & hash 这种高效的索引计算方式，同时也能维持键值对在新数组中的均匀分布。
> 
> 数据迁移
> 
> 在进行扩容操作时，需要将原数组中的所有键值对重新计算索引并迁移到新数组中。由于数组长度是按两倍扩容，原数组中某个位置的元素在新数组中的位置只有两种可能：要么保持不变，要么是原位置加上原数组的长度。
> 
> 例如，原数组长度为 8，某个元素在原数组中的索引为 3。扩容后数组长度变为 16，该元素在新数组中的索引要么还是 3，要么是 3 + 8 = 11。这是因为扩容后，新的 (n - 1) & hash 计算结果只取决于哈希值中新增的那一位。
> 
> 这种特性使得在数据迁移时，不需要重新计算每个元素的哈希值，只需要根据新增的那一位二进制值来判断元素是留在原位置还是移动到新位置，大大减少了数据迁移的复杂度和时间开销。


**扩容条件**：存储的数量大于阈值，

**源码解析**：
1. 在resize()方法中，定义了oldCap参数，记录了原table的长度，定义了newCap参数，记录新table长度，newCap是oldCap长度的2倍（注释1），同时扩展点也乘2。

2. 注释2是循环原table，把原table中的每个链表中的每个元素放入新table。

3. 注释3，e.next==null，指的是链表中只有一个元素，所以直接把e放入新table，其中的e.hash & (newCap - 1)就是计算e在新table中的位置，和JDK1.7中的indexFor()方法是一回事。

4. 注释// preserve order，这个注释是源码自带的，这里定义了4个变量：loHead，loTail，hiHead，hiTail，看起来可能有点眼晕，其实这里体现了JDK1.8对于计算节点在table中下标的新思路：

> 正常情况下，计算节点在table中的下标的方法是：hash&(oldTable.length-1)，扩容之后，table长度翻倍，计算table下标的方法是hash&(newTable.length-1)，也就是hash&(oldTable.length*2-1)，于是我们有了这样的结论：这新旧两次计算下标的结果，要不然就相同，要不然就是新下标等于旧下标加上旧数组的长度。

## 遍历map

```Java
HashMap<String, Object> map = new HashMap<>();
map.put("1", "a");
map.put("4", "d");
map.put("2", "b");
map.put("9", "i");
map.put("3", "c");

Set<Map.Entry<String, Object>> entries = map.entrySet();
for (Map.Entry<String, Object> entry : entries) {
    System.out.println(entry.getKey() + ": " + entry.getValue());
}

System.out.println("-------");

Set<String> keySet = map.keySet();
for (String key : keySet) {
    System.out.println(key + ": " + map.get(key));
}
```

## 与1.7相比变化

1. hash计算规则不同，相比于JDK1.8的hash方法，JDK1.7的hash方法的性能会稍差一点；
2. JDK1.7并没有使用红黑树，如果哈希冲突后，都用链表解决。区别于JDK1.8的尾部插入，JDK1.7采用头部插入的方式；
> 尾插法的问题：多线程同时操作可能导致数据覆盖、
3. JDK1.8在扩容时通过高位运算e.hash & oldCap结果是否为0来确定元素是否需要移动，JDK1.7重新计算了每个元素的哈希值，按旧链表的正序遍历链表、在新链表的头部依次插入，即在转移数据、扩容后，容易出现链表逆序的情况；

### 头插法会导致链表成环

#### 初始链表状态
假设链表初始状态为：`A → B → null`。现有线程 T1 和线程 T2 同时尝试向该链表插入新节点 C 和 D。

#### 正常插入操作步骤

##### 线程 T1 操作步骤
1. **创建新节点**：T1 创建节点 C。
2. **设置指针指向**：T1 将 C 的 `next` 指针指向当前链表头 A，此时链表结构变为 `C → A → B → null`。
3. **更新链表头**：T1 将链表头更新为 C。

##### 线程 T2 操作步骤
1. **创建新节点**：T2 创建节点 D。
2. **设置指针指向**：T2 将 D 的 `next` 指针指向当前链表头 A，此时链表结构变为 `D → A → B → null`。
3. **更新链表头**：T2 将链表头更新为 D。

#### 并发冲突情况

##### 操作交叉执行
如果 T1 和 T2 的操作交叉执行，可能会出现以下操作顺序：
1. T1 将 C 的 `next` 指向 A。
2. T2 将 D 的 `next` 指向 A。
3. T1 将链表头更新为 C。
4. T2 将链表头更新为 D。

#### 链表结构变化
此时，链表可能会形成如下两种看似独立的结构：
- `D → A → B → null`
- `C → A → B → null`

#### 环形结构的形成
由于两个线程都试图将 A 设置为下一个节点，在并发环境下可能会导致链表形成环形结构，例如：
`D → A → B → C → A → B → C → ...`

#### 问题影响
这种环形结构会导致在遍历链表时陷入无限循环，从而引发死循环问题，使得程序的性能受到严重影响，甚至可能导致程序崩溃。 