---
layout: post
title: 我对系统架构设计的理解
---

高并发是线上系统的现状，为了保证高并发请求下的用户可用，需要设计一个支持高可用、高性能、（高）可扩展的系统；

1、高性能：性能体现了系统的并行处理能力，在有限的硬件投入下，提高性能意味着节省成本。同时，性能也反映了用户体验，响应时间分别是100毫秒和1秒，给用户的感受是完全不同的。

2、高可用：表示系统可以正常服务的时间。一个全年不停机、无故障；另一个隔三差五出线上事故、宕机，用户肯定选择前者。另外，如果系统只能做到90%可用，也会大大拖累业务。

3、高扩展：表示系统的扩展能力，流量高峰时能否在短时间内完成扩容，更平稳地承接峰值流量，比如双11活动、明星离婚等热点事件。
<!--more-->


## 如何设计可扩展的系统

### **1. 明确扩展性维度（先定义问题）**

什么样的系统是可扩展的？


- **横向扩展（Scale Out）**：通过增加机器分散负载（如无状态服务）  
- **纵向扩展（Scale Up）**：提升单机性能（如数据库分片）  
- **功能扩展**：通过模块化支持新业务快速接入  

### **2. 分层架构设计（核心方法论）**
```plaintext
┌─────────────────┐
│   客户端层       │ ← CDN/负载均衡（Nginx）
└────────┬────────┘
         ↓
┌─────────────────┐
│   接入层         │ ← API网关（Spring Cloud Gateway）
│   - 限流熔断     │ ← Sentinel
│   - 鉴权         │ ← JWT/OAuth2
└────────┬────────┘
         ↓
┌─────────────────┐
│   业务逻辑层     │ ← 微服务（Spring Cloud）
│   - 竞价服务     │ ← 无状态设计+K8s动态扩缩容
│   - 数据服务     │ ← CQRS模式分离读写
└────────┬────────┘
         ↓
┌─────────────────┐
│   数据层         │ 
│   - 实时数据     │ ← Redis集群+分片
│   - 离线数据     │ ← Hadoop/Spark
└─────────────────┘
```

**关键点**：  
- **无状态化**：Session存储到Redis，服务可任意扩容  
- **异步化**：MQ解耦核心链路（如Kafka处理点击日志）  
- **缓存分级**：本地缓存（Caffeine）+分布式缓存（Redis）  

---

### **3. 扩展性关键技术选型**
| 场景                | 技术方案                          | DSP场景示例                     |
|---------------------|-----------------------------------|--------------------------------|
| 高并发读写          | 数据库分库分表（ShardingSphere）  | 广告订单按广告主ID分片         |
| 实时计算            | Flink窗口计算                     | 实时CTR预估模型更新            |
| 配置动态调整        | Nacos配置中心                     | 竞价算法参数热更新             |
| 全局唯一ID          | 雪花算法（Snowflake）             | 生成竞价请求ID                 |

---

### **4. 可扩展性设计原则**
- **SOLID原则**：特别是接口隔离（如拆分出BiddingService/ReportingService）  
- **防腐层（Anti-Corruption Layer）**：隔离第三方API变更影响  
- **领域驱动设计（DDD）**：通过限界上下文划分微服务边界  

**反例警示**：  
> "我曾见过一个系统将用户画像和竞价逻辑耦合在同一个服务，导致无法单独扩展画像计算资源，后来我们通过DDD重构解耦。"

---

### **5. 实战案例模板**
```plaintext
[项目背景]：XX广告平台日均请求量从10万增长到1亿  
[问题]：原有单体架构导致扩容成本高，竞价延迟飙升  
[解决方案]：  
1. 服务拆分：按业务域拆分为竞价/风控/计费微服务
2. 数据分片：MySQL按广告位ID分库+Redis集群
3. 流量控制：网关层实现基于QPS的动态熔断  
[效果]：TPS从500提升到2万，扩容时间从小时级降到分钟级
```

---

### **6. 高频追问应对**
- **Q：如何避免过度设计？**  
  → "先通过MVP验证核心链路，用指标（如RT/QPS）驱动架构演进，例如我们初期只用Redis分片，后期才引入Elasticsearch做检索。"  

- **Q：微服务拆分过细怎么办？**  
  → "通过SLA分级，核心服务（如竞价）独立部署，非核心服务（如日志）合并部署，并用K8s Namespace隔离。"

---

### **总结回答结构**
1. **分层设计**：展示清晰的架构层次  
2. **技术匹配**：结合DSP业务特点选型  
3. **数据说话**：用性能指标证明设计有效性  
4. **反思能力**：说明权衡取舍的思考过程  



## 如何设计高可用的系统

## 一、高可用系统的定义

一个系统被称为“高可用”，通常意味着其 **可用性 > 99.9%（年宕机时间不超过 8.76 小时）**，甚至要求 99.99%（“四个九”，年宕机时间不到 1小时）。

## 二、方法论框架：高可用的“6字诀”

> **冗余、隔离、容错、监控、弹性、自愈**

每个关键词背后对应一整套设计策略：

---

### 1. 冗余（Redundancy）

> 任何单点都可能导致服务不可用，需要通过冗余来保证“有备份可用”。

#### 技术实践：

* 多副本部署（Nginx、App、DB、Redis）
* 多活架构（跨机房、跨AZ、跨地域）
* 负载均衡（LVS、Nginx、SLB、DNS）

---

### 2. 隔离（Isolation）

> 把“问题”关进笼子，避免“拖垮一片”。

#### 技术实践：

* 服务拆分，微服务架构
* 熔断（Hystrix、Sentinel）、限流（令牌桶、漏桶）
* 多租户资源隔离（线程池、数据库连接池隔离）

---

### 3. 容错（Fault Tolerance）

> 出错不可怕，可怕的是系统崩溃。设计时要假设“组件随时会坏”。

#### 技术实践：

* 超时控制 + 回退机制（Fallback）
* 自动重试（含指数退避）
* 本地缓存 / 预案（降级内容、兜底页面）
* 消息队列保证异步处理

---

### 4. 监控（Monitoring）

> 没有监控，就没有高可用。必须能及时发现问题，追踪根因。

#### 技术实践：

* 指标监控（QPS、RT、错误率）
* 日志监控（ELK、Fluentd）
* 异常告警（Prometheus + Alertmanager、Zabbix、飞书/钉钉推送）
* 分布式链路追踪（Skywalking、Jaeger、Zipkin）

---

### 5. 弹性（Scalability / Elasticity）

> 应对高峰流量，资源要能自动扩展。

#### 技术实践：

* Kubernetes 自动伸缩（HPA）
* 基于流量/CPU/队列长度的自动扩容
* 自动化部署工具（如 ArgoCD、Jenkins）

---

### 6. 自愈（Self-healing）

> 系统出了问题能自动修复，而不是依赖人肉重启。

#### 技术实践：

* 健康检查 + 自动重启（K8s liveness/readiness probe）
* 服务发现 + 实例漂移（Consul、Eureka）
* 崩溃自动重拉容器（K8s Pod CrashLoopBackOff 恢复）

---

## 三、常见系统组件的 HA 设计建议

| 组件    | 高可用方案                                       |
| ----- | ------------------------------------------- |
| Web 层 | Nginx + 多实例部署，K8s Ingress                   |
| 应用层   | 无状态服务，多副本，异步解耦，服务熔断与限流                      |
| 缓存    | Redis Sentinel / Redis Cluster，自动主从切换       |
| 消息队列  | Kafka 多 Broker + ZK，RocketMQ 多主多从 + DLedger |
| 数据库   | MySQL 主从/主主 + VIP漂移 或 MGR/DRDS/PolarDB      |
| 存储    | OSS/S3 + CDN，使用分布式存储（Ceph、HDFS）             |
| 配置中心  | Apollo/Nacos 多节点部署                          |
| 注册中心  | Nacos/Eureka/Consul 多节点部署                   |
| CI/CD | 自动回滚、灰度发布、蓝绿部署                              |

---

## 四、设计流程建议

1. **画出系统组件图**
2. **识别所有单点**
3. **设计每个组件的冗余策略**
4. **定义熔断、限流、降级规则**
5. **接入全链路监控、日志和告警**
6. **演练故障场景（如 Chaos Engineering）**

---

## 五、思维模型（核心问题）

| 问题            | 示例解法                    |
| ------------- | ----------------------- |
| 某个服务挂了怎么办？    | 多副本 + 健康检查 + 自动重启       |
| 网络闪断怎么办？      | 超时 + 重试 + 熔断 + 灾备       |
| 用户数据丢了怎么办？    | 冷热备份 + Binlog 同步 + 日志追踪 |
| 某一类用户流量爆了怎么办？ | 限流 + 降级 + 分布式缓存         |
| 人员误操作怎么办？     | 权限控制 + 审计日志 + 回滚机制      |

---

## 六、总结：高可用不是“买服务”，而是“系统性设计能力”

你可以记住这句口号：

> **避免单点、限制扩散、快速恢复、及时发现、自动修复。**

---

## 七、不得不谈的分布式一致性 (Consensus)

在设计高可用、高扩展系统时，只要涉及状态同步（比如数据库主从、Redis Cluster），就绕不开**一致性**问题。

### 为什么 Paxos 算法这么难懂？

你会发现 Paxos 相关的文章都极其晦涩，原因主要有三点：

1.  **解决的场景太绝望**：Paxos 假设网络是不可靠的（消息会丢、会迟到、会乱序），机器是会随时宕机的，而且**没有全局时钟**。在这么恶劣的环境下达成“共识”，逻辑必然会有大量的 `if-else` 来处理极端异常。
2.  **理论与实现的鸿沟**：算法描述的是一个精简的数学模型（Proposer/Acceptor），但真正写代码时，为了性能（Multi-Paxos）和死锁（活锁）处理，需要填补无数细节（比如如何选主、日志空洞怎么补）。
3.  **讲述方式**：作者 Lamport 最初用古希腊议会的故事来比喻（The Part-Time Parliament），导致大家看了一头雾水。后来才有了《Paxos Made Simple》，但依然全是数学证明。

### 核心流程（极其简化的两阶段）

1.  **Prepare 阶段（拉票）**：
    *   议员 A（Proposer）：大家好，我是编号 100 的议员，我想提议案，大家支持我吗？（不包含具体议案内容）
    *   吃瓜群众（Acceptor）：如果 100 号是目前我见过的最大编号，我就承诺：**不再响应比 100 小的请求**，并把我已经接受过的提案发给你。

2.  **Accept 阶段（投票）**：
    *   议员 A：既然超过半数人理我了，那我正式提出：将 x 设置为 5。
    *   吃瓜群众：如果在这个间隙没有收到编号 > 100 的请求，我就接受这个值。

### 为什么大家现在都用 Raft？

因为 Paxos 太难实现也太难懂，斯坦福的教授设计了 **Raft** 算法。
Raft 的目标非常直接：**可理解性 (Understandability)**。
它把复杂的一致性问题拆解为三个独立的子问题：

1.  **Leader Election（选主）**：谁是话事人？
2.  **Log Replication（日志复制）**：话事人收到的命令，怎么复制给小弟？
3.  **Safety（安全性）**：如何保证数据不丢、不错？

**一句话总结**：
如果面试问 Paxos，你就说它是分布式一致性的基石（Google Chubby 用了它）；实际做系统，大家基本都在用 Raft（Etcd, Consul, TiKV 都在用）。

## 八、Raft 算法图解与实现

Raft 就像一个**严格的帮派管理制度**。在这个帮派里，必须有一个**老大 (Leader)**，其他都是**小弟 (Follower)**。所有的命令（写请求）都必须由老大发话，小弟们照做。

### 1. 三种角色（状态机）

节点在任何时刻只能处于以下三种状态之一：

1.  **Follower（群众）**：
    *   **职责**：默默听话。
    *   **行为**：收不到老大的心跳就变身 Candidate 准备造反。
2.  **Candidate（候选人）**：
    *   **职责**：拉票争当老大。
    *   **行为**：给自己投一票，然后向其他人发“RequestVote”请求。如果拿到大多数票，升级为 Leader。
3.  **Leader（霸主）**：
    *   **职责**：发号施令。
    *   **行为**：不断给小弟发心跳（Heartbeat），告诉大家“我还活着，别造反”。处理所有写请求。

---

### 2. 核心过程一：选主 (Leader Election)

这就好比帮派老大挂了，大家要重选。

1.  **超时机制**：每个 Follower 都有一个闹钟（随机时间 150ms ~ 300ms）。
2.  **触发**：如果闹钟响了还没收到老大消息，Follower 认为老大挂了，于是：
    *   任期（Term） + 1
    *   状态变为 **Candidate**
    *   给自己投一票，并发 RPC 问别人：“选我好不好？”
3.  **投票规则**：
    *   先到先得：一个任期内我只能投给一个人。
    *   择优录取：如果你的日志还没我新，我不投给你。
4.  **当选**：一旦拿到超过半数（N/2 + 1）的票，立马登基，开始疯狂发心跳。

**脑裂情况**：如果有两个人同时参选，票数对半开怎么办？
Raft 很聪明：大家都歇会儿（随机休眠），重置闹钟，谁起得早谁先发起下一轮。

---

### 3. 核心过程二：日志复制 (Log Replication)

这就是“老大发话，小弟记在本子上”的过程。

1.  **Client 提交**：Client 说“把 X 设为 5”。
2.  **Leader 预写**：Leader 先把这条命令记在自己的日志里（此时还没生效，Uncommitted）。
3.  **并行广播**：Leader 发 `AppendEntries` RPC 给所有 Follower：“大哥我这有条新指令，你们也记一下”。
4.  **大多数响应**：
    *   Follower 收到后，写到本地日志，回复“OK”。
    *   一旦 Leader 收到**半数以上**的 OK，这就稳了！
5.  **提交生效 (Commit)**：
    *   Leader 正式执行命令（修改状态机），并告诉 Client“成功”。
    *   Leader 在下一次心跳中告诉 Follower：“刚才那条稳了，你们也可以执行了”。

---

### 4. 为什么 Raft 安全？（Term 任期机制）

**Term（任期）** 是 Raft 的核心概念，相当于朝代。
*   Term 单调递增。
*   **低任期必须服从高任期**。
    *   如果你是旧朝代的老大（Term 1），网络断了。
    *   大家都拥立了新王（Term 2）。
    *   当你网络恢复回来发号施令，大家会甩你一脸 Term 2 的日志：“大清早亡了！”，你只能乖乖变成 Follower。

### 总结 Raft 实现的两个关键 RPC

如果你要手写 Raft，核心就是实现这两个 HTTP/RPC 接口：

1.  `RequestVote`：选我当老大行不行？
2.  `AppendEntries`：
    *   **功能 A**：心跳（我不带数据，就告诉你我活着）。
    *   **功能 B**：同步日志（带上数据让你存）。
