---
title: 分布式与高并发
tags: Architecture
---

分布式锁、全局唯一ID、限流熔断算法实现
<!--more-->

## 分布式锁
分布式锁是一种用于控制分布式系统中多个进程对共享资源的访问的机制。它可以确保在任何给定时间只有一个进程可以访问共享资源，从而避免数据不一致和资源竞争的问题。
### 分布式锁的设计原则
Redis关于分布式锁的定义：
- 互斥（属于安全性）：在任何给定时刻，只有一个客户端可以持有锁。
- 无死锁（属于有效性）：即使锁定资源的客户端崩溃或被分区，也总是可以获得锁；通常通过超时机制实现。
- 容错性（属于有效性）：只要大多数 Redis 节点都启动，客户端就可以获取和释放锁。
### 基于数据库实现


### 基于Redis实现
我会用通俗的语言，像讲故事一样，讲解 **基于 Redis 实现分布式锁的原理**，然后对比 **ZooKeeper 实现分布式锁的异同**，结合前面的 ZooKeeper 分布式锁讲解，把两者的机制和优缺点讲得清楚有趣。假设你对 Redis 和 ZooKeeper 有基础，我会尽量清晰透彻。

#### Redis 是啥？
- **Redis**：
  - 一个高性能的内存键值存储，支持字符串、哈希等数据结构，常用于缓存和锁。

#### 核心思路
- 用 Redis 的 **键值对** 和 **原子操作** 实现锁，像“占坑”：
  - 锁是一个 Key，拿锁是设置 Key，释放是删 Key。
- **比喻**：
  - 像村里水井放个牌子（Key），谁挂牌子谁打水，拿走牌子放锁。

#### 实现步骤
##### (1) 抢锁
- 用 `SETNX`（Set if Not Exists）命令：
  - `SETNX lock_key my_value`：如果 `lock_key` 不存在，设置成功，拿锁。
  - 原子操作，保证只有一个客户端成功。
- 加 **过期时间**（防止死锁）：
  - `EXPIRE lock_key 10`：10 秒后自动释放。
- **优化命令**（Redis 2.6.12+）：
  ```bash
  SET lock_key my_value NX PX 10000
  ```
  - NX：SETNX，PX：毫秒过期（10 秒），一步完成。
- **比喻**：
  - 在水井挂牌子（SETNX），写上名字（my_value），10 秒不拿走自动撤（EXPIRE）。

##### (2) 等待
- 没抢到锁：
  - 轮询（循环 `SETNX`），或用 `WATCH` 监听。
- **比喻**：
  - 看到牌子有名字，等着再试。

##### (3) 释放锁
- 删除 Key：
  ```bash
  DEL lock_key
  ```
- **防误删**：
  - 检查值是不是自己的（`my_value`），用 Lua 脚本：
  ```lua
  if redis.call("GET", KEYS[1]) == ARGV[1] then
      return redis.call("DEL", KEYS[1])
  else
      return 0
  end
  ```
- **比喻**：
  - 拿走牌子前看是不是自己挂的，别拆错人。

#### 代码示例（Java + Jedis）
```java
import redis.clients.jedis.Jedis;

public class RedisDistributedLock {
    private static final String LOCK_KEY = "my_lock";
    private static final String VALUE = "client1"; // 唯一标识

    public static void main(String[] args) {
        Jedis jedis = new Jedis("localhost", 6379);

        // 抢锁
        boolean locked = tryLock(jedis, LOCK_KEY, VALUE, 10);
        if (locked) {
            System.out.println("拿锁成功，干活...");
            try {
                Thread.sleep(2000); // 模拟业务
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
                unlock(jedis, LOCK_KEY, VALUE);
                System.out.println("释放锁");
            }
        } else {
            System.out.println("拿锁失败");
        }
        jedis.close();
    }

    public static boolean tryLock(Jedis jedis, String key, String value, int seconds) {
        String result = jedis.set(key, value, "NX", "EX", seconds);
        return "OK".equals(result);
    }

    public static void unlock(Jedis jedis, String key, String value) {
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then " +
                        "return redis.call('del', KEYS[1]) else return 0 end";
        jedis.eval(script, 1, key, value);
    }
}
```

---

### 2. Redis 和 ZooKeeper 分布式锁的异同
#### (1) 实现原理
- **Redis**：
  - 用键值对（`SETNX`），抢占式，锁是 Key 的存在。
  - **比喻**：挂牌子占水井，谁先挂谁用。
- **ZooKeeper**：
  - 用临时顺序节点（`/locks/lock-0001`），排队式，锁是序号最小。
  - **比喻**：公告板排队，序号小的打水。

#### (2) 一致性
- **Redis**：
  - **弱一致性**：
    - 单节点 Redis 靠内存，集群（Redis Cluster）可能分区不一致。
    - 主从延迟或网络抖动，锁可能错乱。
  - **比喻**：牌子挂错了，别人以为没锁。
- **ZooKeeper**：
  - **强一致性**：
    - ZAB 协议（类 Paxos），多数节点确认，锁状态可靠。
  - **比喻**：公告板多人核对，顺序不乱。

#### (3) 锁释放
- **Redis**：
  - 手动删 Key，超时自动删。
  - **问题**：超时太短，业务没完锁没了；太长，释放慢。
  - **比喻**：牌子时间短被风吹走，时间长占坑。
- **ZooKeeper**：
  - 临时节点，进程挂了自动删，Watch 通知下一位。
  - **优势**：天然防死锁。
  - **比喻**：人走了名字自动擦，下一位自动喊。

#### (4) 性能
- **Redis**：
  - **高性能**：
    - 内存操作，单次锁几毫秒，适合高频场景。
  - **比喻**：挂牌子快如闪电。
- **ZooKeeper**：
  - **中性能**：
    - 写磁盘 + 同步，锁延迟几十毫秒，适合低频高一致。
  - **比喻**：公告板写慢点但稳。

#### (5) 复杂性
- **Redis**：
  - **简单**：
    - SETNX + Lua 脚本，几行搞定。
  - **问题**：自己管超时、重试。
  - **比喻**：挂牌子简单，但得看表。
- **ZooKeeper**：
  - **复杂**：
    - 顺序节点 + Watch，逻辑多。
  - **优势**：Curator 封装省心。
  - **比喻**：排队麻烦，但系统管好。

#### (6) 容错
- **Redis**：
  - 单点故障（需哨兵/集群），宕机锁丢。
  - **比喻**：牌子被风吹跑，水井没人管。
- **ZooKeeper**：
  - 集群容错，少数节点挂没事。
  - **比喻**：公告板多村共管，一村倒闭不乱。

---

### 3. 对比总结表
| 特性           | Redis                     | ZooKeeper                |
|----------------|---------------------------|--------------------------|
| **原理**       | SETNX 抢占式             | 顺序节点排队式          |
| **一致性**     | 弱一致（单点/集群问题）  | 强一致（ZAB 协议）      |
| **释放**       | 手动删 + 超时            | 自动删（临时节点）      |
| **性能**       | 高（毫秒级）             | 中（几十毫秒）          |
| **复杂性**     | 简单（需自己管）         | 复杂（框架帮管）        |
| **容错**       | 单点风险                 | 集群高可用              |
| **场景**       | 高频、短时锁             | 低频、高一致锁          |

---

### 4. 通俗总结
- **Redis 锁**：
  - 靠占坑（SETNX），快但不稳，像挂牌子抢水井。
- **ZooKeeper 锁**：
  - 靠排队（顺序节点），稳但慢，像公告板管秩序。
- **比喻**：
  - Redis 是“快抢水桶”，ZooKeeper 是“稳排队”，看你急不急。

---

### 5. 检查理解
- Redis 咋防死锁？
- ZooKeeper 咋保证顺序？
- 你会选哪个锁库存？

### 基于Zookeeper实现
我会用通俗的语言，像讲故事一样，讲解 **如何使用 ZooKeeper 实现分布式锁**，从原理到具体步骤，再给个代码示例，帮你彻底搞清楚。假设你对 ZooKeeper 有基础（比如知道它是分布式协调服务），我会尽量清晰有趣，把过程讲透。

---

### 1. 分布式锁和 ZooKeeper 的关系
#### 分布式锁是啥？
- **分布式锁**：
  - 在多台机器（分布式系统）上，保证同一时刻只有一个进程/线程能访问共享资源（比如数据库、文件）。
- **比喻**：
  - 像村里只有一个水井，大家轮流打水，锁就是“谁拿水桶谁打水”的规矩。

#### 为啥用 ZooKeeper？
- **ZooKeeper 特点**：
  - 提供一致性（强一致性，数据同步快）。
  - 支持临时顺序节点（Ephemeral Sequential），天然适合锁。
- **比喻**：
  - ZooKeeper 是“村里的公告板”，记录谁在排队打水，谁拿锁。

---

### 2. ZooKeeper 实现分布式锁的原理
ZooKeeper 用它的 **临时顺序节点** 和 **Watch 机制** 实现锁，核心思路是“排队拿锁”。

#### 核心概念
- **临时顺序节点**：
  - 节点名自动带序号（比如 `/lock-0000000001`），创建后顺序递增。
  - 临时节点：进程挂了，节点自动删，锁释放。
- **Watch**：
  - 监听前一个节点，前面的人释放锁，轮到我。

#### 锁的流程
1. **抢锁**：
   - 每个进程在 ZooKeeper 的锁路径（比如 `/locks`）下创建临时顺序节点。
   - 检查自己序号是不是最小的，最小就拿锁。
2. **等待**：
   - 不是最小的，监听前一个节点（Watch）。
3. **释放**：
   - 拿锁的进程干完活，删自己节点，后面的依次接替。
- **比喻**：
  - 村民去公告板排队写名字（节点），序号最小的打水，后面的人盯着前面的，等他走就上。

---

### 3. 具体实现步骤
#### 假设场景
- 3 个进程（P1、P2、P3）抢锁，访问共享资源。

#### 步骤
1. **创建锁节点**：
   - P1 在 `/locks` 下创建 `/locks/lock-0000000001`。
   - P2 创建 `/locks/lock-0000000002`。
   - P3 创建 `/locks/lock-0000000003`。
2. **检查序号**：
   - 每个进程查 `/locks` 下的子节点列表：
     - P1 看到 `[0001, 0002, 0003]`，自己最小，拿锁。
     - P2 看到 `[0001, 0002, 0003]`，不是最小，监听 `/locks/lock-0000000001`。
     - P3 看到 `[0001, 0002, 0003]`，监听 `/locks/lock-0000000002`。
3. **执行业务**：
   - P1 拿锁，干活（比如写数据库）。
4. **释放锁**：
   - P1 干完，删 `/locks/lock-0000000001`。
   - P2 的 Watch 触发，发现自己最小（`[0002, 0003]`），拿锁。
5. **依次轮流**：
   - P2 删节点，P3 接手。
- **比喻**：
  - 村民排队，1 号打水，2 号盯着 1 号，1 号走后 2 号上。

---

### 4. Java 代码示例
用 Apache Curator（ZooKeeper 的高级客户端）实现，简单又可靠。

#### 依赖
```xml
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>5.2.0</version>
</dependency>
```

#### 代码
```java
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.locks.InterProcessMutex;
import org.apache.curator.retry.ExponentialBackoffRetry;

public class ZkDistributedLock {
    public static void main(String[] args) throws Exception {
        // 连接 ZooKeeper
        CuratorFramework client = CuratorFrameworkFactory.newClient(
            "localhost:2181", // ZooKeeper 地址
            new ExponentialBackoffRetry(1000, 3) // 重试策略
        );
        client.start();

        // 创建分布式锁
        InterProcessMutex lock = new InterProcessMutex(client, "/locks/my-lock");

        // 模拟多个进程抢锁
        Runnable task = () -> {
            try {
                System.out.println(Thread.currentThread().getName() + " 尝试获取锁...");
                lock.acquire(); // 获取锁，阻塞直到成功
                System.out.println(Thread.currentThread().getName() + " 获取到锁，开始干活...");
                Thread.sleep(2000); // 模拟业务
                System.out.println(Thread.currentThread().getName() + " 干完活，释放锁...");
                lock.release(); // 释放锁
            } catch (Exception e) {
                e.printStackTrace();
            }
        };

        // 启动 3 个线程模拟
        Thread t1 = new Thread(task, "P1");
        Thread t2 = new Thread(task, "P2");
        Thread t3 = new Thread(task, "P3");
        t1.start();
        t2.start();
        t3.start();

        t1.join();
        t2.join();
        t3.join();
        client.close();
    }
}
```

#### 输出（示例）
```
P1 尝试获取锁...
P1 获取到锁，开始干活...
P2 尝试获取锁...
P3 尝试获取锁...
P1 干完活，释放锁...
P2 获取到锁，开始干活...
P2 干完活，释放锁...
P3 获取到锁，开始干活...
P3 干完活，释放锁...
```
- **说明**：
  - P1 先拿锁，P2、P3 等待，P1 释放后 P2 上，依次排队。

---

### 5. 实现细节解析
#### Curator 做了啥？
- **InterProcessMutex**：
  - 内部用 ZooKeeper 创建临时顺序节点（`/locks/my-lock/lock-xxx`）。
  - 检查自己是不是最小节点。
  - 监听前一个节点，自动排队。
- **容错**：
  - 进程挂了，临时节点自动删，锁释放。

#### 手动实现（伪代码）
如果不用 Curator，自己写：
```java
// 连接 ZooKeeper
ZkClient zk = new ZkClient("localhost:2181");

// 抢锁
String node = zk.createEphemeralSequential("/locks/my-lock-", "data");
List<String> nodes = zk.getChildren("/locks");
Collections.sort(nodes);
String minNode = nodes.get(0);
if (node.equals("/locks/" + minNode)) {
    // 拿锁成功，干活
} else {
    // 监听前一个节点
    String prevNode = nodes.get(nodes.indexOf(node.split("/")[2]) - 1);
    zk.watch(prevNode, () -> {
        // 前一个删了，重新检查
    });
}

// 释放锁
zk.delete(node);
```

---

### 6. 注意事项
- **性能**：
  - ZooKeeper 锁适合低频、高一致性场景，高频抢锁（每秒千次）可能慢。
- **超时**：
  - 加锁超时（`lock.acquire(5, TimeUnit.SECONDS)`），防死锁。
- **比喻**：
  - 锁是水井钥匙，ZooKeeper 是排队簿，别抢太猛，村里人扛不住。

---

### 7. 通俗总结
- **怎么用**：
  - 在 ZooKeeper 下建临时顺序节点，排队拿锁，小序号干活，删节点放锁。
- **原理**：
  - 像村民排队打水，公告板（ZooKeeper）管顺序。
- **比喻**：
  - ZooKeeper 是“智能排队机”，谁排第一谁拿水桶（锁）。

---

### 8. 检查理解
- 临时顺序节点咋排队的？
- Watch 咋帮拿锁？
- 你会用它锁啥资源？

## 分布式系统下的全局唯一ID
可以分为两个大类：
- 类DB的自增ID
- 类snowflak
### UUID
UUID（Universally Unique Identifier）是一种标准的标识符，由128位数字组成，通常表示为32个十六进制数字的字符串。UUID的唯一性由其生成算法保证，不同的UUID之间几乎不可能重复。
#### UUID生成算法
UUID的生成算法基于以下几个因素：
1. 时间戳：使用当前时间的毫秒数作为UUID的一部分。
2. 时钟序列：在同一毫秒内生成多个UUID时，使用时钟序列进行区分。
3. 节点标识符：使用计算机的网络地址作为UUID的一部分。
#### UUID的唯一性
UUID的唯一性由其生成算法保证，不同的UUID之间几乎不可能重复。然而，UUID的生成算法也存在一些问题，例如：
1. 时间戳的唯一性受到系统时钟的影响，可能会出现时间戳回拨的情况。
2. 节点标识符的唯一性受到网络环境的影响，可能会出现节点标识符重复的情况。
#### 总结
优点：
- 本地生成没有网络消耗
缺点：
- 长度过长，容易造成数据库性能瓶颈、不适合高并发场景
- 无序性，无法判断生成ID的先后顺序

### Snowflake算法
第1位占用1bit，其值始终是0，可看做是符号位不使用。第2位开始的41位是时间戳，41-bit位可表示2^41个数，每个数代表毫秒，那么雪花算法可用的时间年限是(1L<<41)/(1000L360024*365)=69 年的时间。中间的10-bit位可表示机器数，即2^10 = 1024台机器，但是一般情况下我们不会部署这么台机器。如果我们对IDC（互联网数据中心）有需求，还可以将 10-bit 分 5-bit 给 IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，具体的划分可以根据自身需求定义。最后12-bit位是自增序列，可表示2^12 = 4096个数。


Snowflake 算法结构（64位）：
- 1位符号位：始终为0
- 41位时间戳：精确到毫秒，可使用69年
- 10位工作机器ID：
  - 5位数据中心ID（0-31）
  - 5位机器ID（0-31）
- 12位序列号：同一毫秒内可生成4096个ID

Snowflake 保证唯一性和有序性的机制：
1. 时间戳保证全局趋势递增
2. 工作机器ID保证不同节点生成的ID不重复
3. 序列号保证同一毫秒内的唯一性
4. 时钟回拨问题解决：
   - 记录上一次的时间戳
   - 发现回拨时等待或抛出异常
   - 使用备用生成器

**优点**：

1. 高性能：本地生成，无网络消耗
2. 有序性：基于时间戳，天然有序
3. 可配置：机器标识可根据需求调整
4. ID紧凑：64位整型，占用空间小

**缺点**：

1. 依赖机器时钟，时钟回拨会导致问题
2. 机器ID需要手动配置和维护

## 限流算法


### 固定窗口限流算法
> Fixed Window Rate Limiting Algorithm，将时间划分为固定大小的窗口，在每个窗口（单位时间）内限制请求数量。

**工作原理**：
- 将时间划分为固定长度的窗口（如1秒）
- 在每个窗口内维护一个计数器
- 每次请求到达时：
  1. 判断当前时间窗口
  2. 查看计数器是否超过阈值
  3. 未超过则请求通过并计数器+1
  4. 超过则拒绝请求
- 当进入新的时间窗口时，计数器重置为0

**示例代码**：
```java
public class FixedWindowRateLimiter {
    // 时间窗口大小（毫秒）
    private long windowSize;
    // 窗口内允许的最大请求数
    private int maxRequests;
    // 当前窗口的请求计数
    private AtomicInteger count;
    // 当前窗口的开始时间
    private AtomicLong windowStart;

    public FixedWindowRateLimiter(long windowSize, int maxRequests) {
        this.windowSize = windowSize;
        this.maxRequests = maxRequests;
        this.count = new AtomicInteger(0);
        this.windowStart = new AtomicLong(System.currentTimeMillis());
    }

    public boolean tryAcquire() {
        long now = System.currentTimeMillis();
        long currentWindow = windowStart.get();
        // 检查是否需要重置时间窗口
        if (now - currentWindow > windowSize) {
            count.set(0);
            windowStart.set(now);
        }
        // 尝试增加计数
        if (count.get() < maxRequests) {
            count.incrementAndGet();
            return true;
        }
        return false;
    }
}

public class RateLimiterDemo {
    public static void main(String[] args) {
        // 创建一个1秒内最多允许100个请求的限流器
        FixedWindowRateLimiter limiter = new FixedWindowRateLimiter(1000, 100);

        // 模拟请求
        for (int i = 0; i < 150; i++) {
            boolean allowed = limiter.tryAcquire();
            System.out.println("请求-" + i + ": " + (allowed ? "通过" : "被限流"));
        }
    }
}
```

**优点**：
- 实现简单，容易理解
- 内存占用小
- 适合简单场景

**缺点**：
- 临界问题：两个窗口交界处可能突发双倍流量
  > 假设限制是每秒100个请求
  > - 第一个窗口的最后500ms涌入100个请求
  > - 第二个窗口的开始500ms又涌入100个请求
  > - 结果在这1秒内（跨越两个窗口）实际通过了200个请求
  > - 这违背了每秒限制100个请求的初衷
- 突发流量：窗口开始时可能瞬间被占满
- 资源利用不均匀
- 突发流量：窗口开始时可能瞬间被占满
- 资源利用不均匀

使用场景：
- 简单的接口限流
- 并发量不大的场景
- 对突发流量不敏感的场景

### 滑动窗口算法
Sliding Window Rate Limiting Algorithm，通过滑动时间窗口的方式，解决固定窗口算法的临界突发问题。

**工作原理**：
- 将时间窗口细分为多个小窗口（如：1s分为10个100ms的小窗口）
- 每个小窗口独立计数
- 总请求数为当前时间往前一个完整时间窗口内的所有小窗口计数之和
- 随着时间推移，窗口平滑滑动，最老的小窗口数据会被丢弃

**改进效果**：
1. 解决临界问题
   - 固定窗口：在窗口切换时可能允许2倍的请求量
   - 滑动窗口：任意时间点都只统计最近一个时间窗口的请求量
   > 例如：限制每秒100个请求
   > - 滑动窗口会把1s分成多个小窗口（如10个）
   > - 每100ms一个小窗口，每个小窗口最多允许10个请求
   > - 任意时刻向前看1s的请求总和都不会超过100

2. 平滑限流效果
   - 更精确的流量控制
   - 避免突发流量
   - 资源使用更均匀

3. 更好的实时性
   - 窗口连续滑动
   - 细粒度的统计和控制
   - 及时响应流量变化

**示例代码**:
```java
public class SlidingWindowRateLimiter {
    // 窗口总时间长度（毫秒）
    private long windowSize;
    // 小窗口数量
    private int subWindowCount;
    // 总请求限制数
    private int maxRequests;
    // 使用数组记录每个小窗口的请求数
    private AtomicInteger[] subWindows;
    // 当前小窗口索引
    private AtomicInteger currentIndex;
    // 上次滑动时间
    private AtomicLong lastSlideTime;

    public SlidingWindowRateLimiter(long windowSize, int subWindowCount, int maxRequests) {
        this.windowSize = windowSize;
        this.subWindowCount = subWindowCount;
        this.maxRequests = maxRequests;
        this.subWindows = new AtomicInteger[subWindowCount];
        for (int i = 0; i < subWindowCount; i++) {
            subWindows[i] = new AtomicInteger(0);
        }
        this.currentIndex = new AtomicInteger(0);
        this.lastSlideTime = new AtomicLong(System.currentTimeMillis());
    }

    public boolean tryAcquire() {
        long now = System.currentTimeMillis();
        // 计算需要滑动的窗口数
        int slideCount = (int) ((now - lastSlideTime.get()) 
            / (windowSize / subWindowCount));
        if (slideCount > 0) {
            slideWindow(slideCount);
            lastSlideTime.addAndGet(slideCount * (windowSize / subWindowCount));
        }

        // 计算当前总请求数
        int totalRequests = 0;
        for (AtomicInteger subWindow : subWindows) {
            totalRequests += subWindow.get();
        }

        // 如果未超过限制，当前小窗口计数加1
        if (totalRequests < maxRequests) {
            subWindows[currentIndex.get()].incrementAndGet();
            return true;
        }
        return false;
    }

    private void slideWindow(int slideCount) {
        for (int i = 0; i < slideCount && i < subWindowCount; i++) {
            int index = (currentIndex.get() + 1) % subWindowCount;
            subWindows[index].set(0);
            currentIndex.set(index);
        }
    }
}

// 使用示例
public class SlidingWindowDemo {
    public static void main(String[] args) {
        // 创建一个1秒内最多允许100个请求的限流器，分成10个小窗口
        SlidingWindowRateLimiter limiter = 
            new SlidingWindowRateLimiter(1000, 10, 100);

        // 模拟请求
        for (int i = 0; i < 150; i++) {
            boolean allowed = limiter.tryAcquire();
            System.out.println("请求-" + i + ": " + (allowed ? "通过" : "被限流"));
            try {
                Thread.sleep(5);  // 模拟请求间隔
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }
}
```

**缺点**：
- 实现相对复杂
- 需要存储多个小窗口的数据
- 内存占用较大

### 漏斗算法
Leaky Bucket Algorithm，把请求比作水，漏斗比作处理程序。水（请求）从上方倒入漏斗，漏斗以固定速率出水（处理请求）。当水流入速率过大时，漏斗会溢出（拒绝请求）。

**工作原理**：
1. 漏斗容量：最大可容纳的请求数
2. 漏水速率：固定的请求处理速率
3. 请求处理：
   - 请求到达时放入漏斗
   - 漏斗按固定速率处理请求
   - 漏斗满时拒绝新请求
   - 漏斗空时等待新请求

**示例代码**：
```java
public class LeakyBucketRateLimiter {
    // 漏斗容量
    private final int capacity;
    // 漏水速率（每秒处理请求数）
    private final int rate;
    // 当前水量
    private final AtomicInteger water;
    // 上次漏水时间
    private final AtomicLong lastLeakTime;

    public LeakyBucketRateLimiter(int capacity, int rate) {
        this.capacity = capacity;
        this.rate = rate;
        this.water = new AtomicInteger(0);
        this.lastLeakTime = new AtomicLong(System.currentTimeMillis());
    }

    public boolean tryAcquire() {
        long now = System.currentTimeMillis();
        // 计算从上次漏水到现在应该漏掉多少水
        int leakedWater = (int) ((now - lastLeakTime.get()) * rate / 1000);
        if (leakedWater > 0) {
            // 更新水量和上次漏水时间
            int currentWater = Math.max(0, water.get() - leakedWater);
            water.set(currentWater);
            lastLeakTime.set(now);
        }

        // 尝试加水，如果未超过容量则请求通过
        if (water.get() < capacity) {
            water.incrementAndGet();
            return true;
        }
        return false;
    }
}
```

**优点**：
   
   - 固定处理速率，稳定性好
   - 削峰填谷效果明显
   - 适合流量整形场景

**缺点**：   
   - 实现复杂度较高
   - 响应延迟可能较大
   - 不适合突发流量处理

**使用场景**：
- 需要严格控制处理速率的场景
- 消息队列处理
- 网络流量整形
- 数据库写入控制

### 令牌桶算法
Token Bucket Algorithm，以恒定速率向桶中放入令牌，请求获取到令牌才能被处理。当桶满时，新增的令牌会被丢弃。

**工作原理**：
1. 令牌生成：系统以固定速率向桶中放入令牌
2. 令牌消费：
   - 请求到达时尝试获取令牌
   - 有令牌则处理请求并消耗令牌
   - 无令牌则请求被拒绝
3. 突发处理：
   - 桶中可以缓存一定数量的令牌
   - 允许一定程度的突发流量
   - 最大突发量由桶容量决定

**示例代码**：
```java
public class TokenBucketRateLimiter {
    // 令牌桶容量
    private final int capacity;
    // 令牌生成速率（每秒）
    private final int rate;
    // 当前令牌数量
    private final AtomicInteger tokens;
    // 上次令牌生成时间
    private final AtomicLong lastTokenTime;

    public TokenBucketRateLimiter(int capacity, int rate) {
        this.capacity = capacity;
        this.rate = rate;
        this.tokens = new AtomicInteger(0);
        this.lastTokenTime = new AtomicLong(System.currentTimeMillis());
    }

    public boolean tryAcquire() {
        long now = System.currentTimeMillis();
        // 生成新令牌
        int newTokens = (int) ((now - lastTokenTime.get()) * rate / 1000);
        if (newTokens > 0) {
            // 更新令牌数量，不超过容量
            int currentTokens = Math.min(capacity, 
                tokens.get() + newTokens);
            tokens.set(currentTokens);
            lastTokenTime.set(now);
        }

        // 尝试获取令牌
        if (tokens.get() > 0) {
            tokens.decrementAndGet();
            return true;
        }
        return false;
    }
}

// 使用示例
public class TokenBucketDemo {
    public static void main(String[] args) {
        // 创建容量为10，每秒生成5个令牌的限流器
        TokenBucketRateLimiter limiter = 
            new TokenBucketRateLimiter(10, 5);

        // 模拟突发请求
        for (int i = 0; i < 20; i++) {
            boolean allowed = limiter.tryAcquire();
            System.out.println("请求-" + i + ": " + 
                (allowed ? "通过" : "被限流"));
            try {
                Thread.sleep(100);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }
}
```

**优点**：
1. 支持突发流量
   - 令牌桶可以缓存令牌
   - 短时间内可以处理大量请求
   - 更符合实际业务场景

2. 平滑限流
   - 令牌生成速率恒定
   - 长期来看可以限制平均处理速率
   - 避免资源使用过于波动

3. 灵活配置
   - 可以通过调整桶容量控制突发量
   - 可以通过调整生成速率控制平均速率
   - 适应不同场景需求

**缺点**：
- 令牌生成和消费的计算可能会有误差
- 分布式环境下同步令牌状态比较困难
- 需要额外的存储空间维护令牌计数

**使用场景**：
1. API限流
   - 允许临时突发但限制平均访问率
   - 适合大部分Web API场景

2. 资源访问控制
   - 数据库连接池管理
   - 文件系统访问控制
   - 网络带宽限制

3. 服务降级
   - 削峰填谷
   - 保护后端服务
   - 合理分配资源

令牌桶算法是目前使用最广泛的限流算法，它在保证系统稳定的同时，也提供了一定的灵活性，能够很好地平衡系统的可用性和保护性。

## 高并发写请求处理策略：

1. 数据库优化
   - 分库分表：水平拆分、垂直拆分
   - 读写分离：主库写入，从库读取
   - 批量写入：合并多个写操作
   - 索引优化：避免过多索引，合理设计索引
   - 连接池管理：合理配置连接数

2. 缓存策略
   - 写缓存：先写缓存，异步写库
   - 双写一致性：缓存和数据库同步更新
   - 缓存预热：提前加载热点数据
   - 缓存降级：高峰期只更新缓存

3. 异步处理
   - 消息队列：削峰填谷
   - 异步写入：非核心数据延迟写入
   - 事件驱动：解耦业务流程
   - 任务队列：将写操作转为任务执行

4. 架构优化
   - 服务拆分：按业务领域拆分
   - 集群部署：负载均衡
   - 分布式事务：保证数据一致性
   - 熔断降级：保护核心业务

5. 并发控制
   - 分布式锁：避免重复写入
   - 乐观锁：版本号控制
   - 悲观锁：必要时加锁保护
   - 限流：合理控制写入速率

## AI工具